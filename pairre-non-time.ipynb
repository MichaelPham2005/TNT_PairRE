{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14321783,"sourceType":"datasetVersion","datasetId":9142735},{"sourceId":14329493,"sourceType":"datasetVersion","datasetId":9148298},{"sourceId":14381972,"sourceType":"datasetVersion","datasetId":9184802},{"sourceId":14382068,"sourceType":"datasetVersion","datasetId":9184870},{"sourceId":14382433,"sourceType":"datasetVersion","datasetId":9185124}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":11218.194769,"end_time":"2025-12-29T12:06:45.017451","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-12-29T08:59:46.822682","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"06e7570c","cell_type":"code","source":"import shutil\nimport os\n\n# Đường dẫn gốc trong Input (Thay 'ten-dataset-cua-ban' bằng tên thực tế)\n# Bạn có thể xem đường dẫn chính xác ở thanh sidebar bên phải phần \"Data\"\nsource_dir = '/kaggle/input/pairre-nt2/baseline_icews14'\n\n# Đường dẫn đích (Nơi bạn sẽ chạy code)\ndestination_dir = '/kaggle/working/my_code'\n\n# Copy toàn bộ thư mục\nif not os.path.exists(destination_dir):\n    shutil.copytree(source_dir, destination_dir)\n    print(\"Đã copy code sang /kaggle/working thành công!\")\nelse:\n    print(\"Thư mục code đã tồn tại ở working directory.\")\n\n# Di chuyển thư mục làm việc hiện tại vào đó để chạy lệnh terminal cho dễ\nos.chdir(destination_dir)\nprint(f\"Thư mục làm việc hiện tại: {os.getcwd()}\")","metadata":{"_cell_guid":"61ba90af-8af1-4a25-85bc-35dd26d79eab","_uuid":"e69a5724-0b49-454d-a184-1e1ec5bbe0ba","collapsed":false,"execution":{"iopub.status.busy":"2026-01-03T14:46:44.484958Z","iopub.execute_input":"2026-01-03T14:46:44.485769Z","iopub.status.idle":"2026-01-03T14:46:44.522371Z","shell.execute_reply.started":"2026-01-03T14:46:44.485732Z","shell.execute_reply":"2026-01-03T14:46:44.521862Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.125121,"end_time":"2025-12-29T08:59:49.401488","exception":false,"start_time":"2025-12-29T08:59:49.276367","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Đã copy code sang /kaggle/working thành công!\nThư mục làm việc hiện tại: /kaggle/working/my_code\n","output_type":"stream"}],"execution_count":1},{"id":"9a02f50b","cell_type":"code","source":"import torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-03T14:06:24.264101Z","iopub.execute_input":"2026-01-03T14:06:24.264547Z","iopub.status.idle":"2026-01-03T14:06:27.981691Z","shell.execute_reply.started":"2026-01-03T14:06:24.264526Z","shell.execute_reply":"2026-01-03T14:06:27.980965Z"},"papermill":{"duration":3.76909,"end_time":"2025-12-29T08:59:53.172796","exception":false,"start_time":"2025-12-29T08:59:49.403706","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"PyTorch version: 2.6.0+cu124\nCUDA available: True\n","output_type":"stream"}],"execution_count":5},{"id":"9ed4250f","cell_type":"code","source":"%%writefile requirements.txt\ntqdm\nnumpy\nscikit-learn\nscipy","metadata":{"execution":{"iopub.status.busy":"2026-01-03T14:46:47.619333Z","iopub.execute_input":"2026-01-03T14:46:47.619873Z","iopub.status.idle":"2026-01-03T14:46:47.624680Z","shell.execute_reply.started":"2026-01-03T14:46:47.619843Z","shell.execute_reply":"2026-01-03T14:46:47.624055Z"},"papermill":{"duration":0.00881,"end_time":"2025-12-29T08:59:53.183944","exception":false,"start_time":"2025-12-29T08:59:53.175134","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Writing requirements.txt\n","output_type":"stream"}],"execution_count":2},{"id":"a28d53ed","cell_type":"code","source":"%%capture\n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2026-01-03T14:47:05.408896Z","iopub.execute_input":"2026-01-03T14:47:05.409616Z","iopub.status.idle":"2026-01-03T14:47:08.700629Z","shell.execute_reply.started":"2026-01-03T14:47:05.409579Z","shell.execute_reply":"2026-01-03T14:47:08.699814Z"},"papermill":{"duration":4.003516,"end_time":"2025-12-29T08:59:57.189514","exception":false,"start_time":"2025-12-29T08:59:53.185998","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":4},{"id":"7f9d5f67-16d3-4450-bd2a-a2ce4707bbe6","cell_type":"code","source":"!bash /kaggle/working/my_code/download_dataset.sh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:47:14.134784Z","iopub.execute_input":"2026-01-03T14:47:14.135457Z","iopub.status.idle":"2026-01-03T14:47:17.667030Z","shell.execute_reply.started":"2026-01-03T14:47:14.135409Z","shell.execute_reply":"2026-01-03T14:47:17.666344Z"}},"outputs":[{"name":"stdout","text":"==================================================\nDownloading ICEWS14 Dataset\n==================================================\nDownloading from https://dl.fbaipublicfiles.com/tkbc/data.tar.gz ...\ndata.tar.gz         100%[===================>]  72.90M   268MB/s    in 0.3s    \nExtracting data.tar.gz ...\nChecking archive contents...\nAvailable datasets:\ntotal 24\ndrwxrwxr-x 6 1185300502 1185300502 4096 Jan  2  2020 .\ndrwxr-xr-x 3 root       root       4096 Jan  3 14:47 ..\ndrwxrwxr-x 2 1185300502 1185300502 4096 Jan  2  2020 ICEWS05-15\ndrwxrwxr-x 2 1185300502 1185300502 4096 Jan  2  2020 ICEWS14\ndrwxrwxr-x 2 1185300502 1185300502 4096 Apr  8  2020 wikidata\ndrwxrwxr-x 2 1185300502 1185300502 4096 Jan  2  2020 yago15k\nFound: src_data/ICEWS14\n\nContents of src_data/ICEWS14/:\ntotal 6.0M\n-rw-rw-r-- 1 1185300502 1185300502   99 Jan  2  2020 LICENSE\n-rw-rw-r-- 1 1185300502 1185300502 604K Jan  2  2020 test\n-rw-rw-r-- 1 1185300502 1185300502 4.8M Jan  2  2020 train\n-rw-rw-r-- 1 1185300502 1185300502 601K Jan  2  2020 valid\n\nFound files without extension - copying to raw/ with .txt extension...\n✓ Copied train, valid, test → raw/*.txt\n\nPreprocessing data to create .pkl files...\n============================================================\nICEWS14 Data Preprocessing\n============================================================\n\n1. Reading raw data...\n  Train: 72826 triplets\n  Valid: 8941 triplets\n  Test:  8963 triplets\n\n2. Building entity/relation mappings...\n  Entities: 7128\n  Relations: 230\n\n3. Normalizing timestamps...\n  Original range: [1388534400.0, 1419984000.0]\n  Normalized to: [0.0, 1.0]\n\n4. Converting to ID format...\n\n5. Building temporal filter (to_skip.pkl)...\n  Unique (h,r,t) facts: 50295\n\n6. Saving processed data...\n\n✓ Processed data saved to: processed/\n  - train.pkl (72826 samples)\n  - valid.pkl (8941 samples)\n  - test.pkl (8963 samples)\n  - mappings.pkl (entity/relation IDs)\n  - to_skip.pkl (temporal filtering)\n\n============================================================\nPreprocessing Complete!\n============================================================\nYou can now run training:\n  python run.py --do_train --cuda --data_path processed ...\n============================================================\n\n✓ Preprocessing complete!\n\n✓ Data ready in ./processed/\n\nContents:\ntotal 3.5M\n-rw-r--r-- 1 root root 259K Jan  3 14:47 mappings.pkl\n-rw-r--r-- 1 root root 175K Jan  3 14:47 test.pkl\n-rw-r--r-- 1 root root 1.5M Jan  3 14:47 to_skip.pkl\n-rw-r--r-- 1 root root 1.4M Jan  3 14:47 train.pkl\n-rw-r--r-- 1 root root 175K Jan  3 14:47 valid.pkl\n\n==================================================\nDataset Ready!\n==================================================\nYou can now run training:\n  bash train_baseline.sh\n  OR\n  python run.py --do_train --cuda --data_path processed ...\n==================================================\n","output_type":"stream"}],"execution_count":5},{"id":"668b7103-39ec-4c23-a63d-b61ff80c6cd0","cell_type":"code","source":"%%writefile /kaggle/working/my_code/train_baseline.sh\n#!/bin/bash\n\n# Baseline PairRE Training Script for ICEWS14\n# NO temporal modeling - timestamps ignored!\n\necho \"==================================================\"\necho \"BASELINE PairRE - Static KG Approach\"\necho \"Dataset: ICEWS14 (ignoring timestamps)\"\necho \"==================================================\"\n\n# Step 1: Download and prepare data (if not exists)\nif [ ! -d \"processed\" ]; then\n    echo \"\"\n    echo \"Downloading ICEWS14 dataset from Facebook AI Research...\"\n    bash download_dataset.sh\n    \n    if [ $? -ne 0 ]; then\n        echo \"❌ Data download failed!\"\n        exit 1\n    fi\nelse\n    echo \"✓ Data already prepared in ./processed\"\nfi\n\n# Configuration\nDATA_PATH=\"processed\"\nMODEL=\"BaselinePairRE\"\nSAVE_DIR=\"checkpoints/ICEWS14_BaselinePairRE\"\n\n# Hyperparameters (matching temporal model for fair comparison)\nDIMENSION=500\nGAMMA=12.0\nLR=0.0001\nBATCH_SIZE=256\nNEG_SIZE=128\nADV_TEMP=1.0\nREG=0.000001\nMAX_STEPS=100000\nWARMUP=50000\nVALID_STEPS=5000\nSAVE_STEPS=5000\n\necho \"\"\necho \"Configuration:\"\necho \"  Model: $MODEL\"\necho \"  Dimension: $DIMENSION\"\necho \"  Gamma: $GAMMA\"\necho \"  Learning Rate: $LR\"\necho \"  Batch Size: $BATCH_SIZE\"\necho \"  Negative Samples: $NEG_SIZE\"\necho \"  Max Steps: $MAX_STEPS\"\necho \"\"\necho \"NOTE: This baseline IGNORES timestamps during training!\"\necho \"      Evaluation uses temporal filtering for fair comparison.\"\necho \"\"\n\n# Run training\npython -u run.py \\\n  --do_train \\\n  --cuda \\\n  --do_valid \\\n  --do_test \\\n  --evaluate_train \\\n  --model $MODEL \\\n  --data_path $DATA_PATH \\\n  -n $NEG_SIZE \\\n  -b $BATCH_SIZE \\\n  -d $DIMENSION \\\n  -g $GAMMA \\\n  -a $ADV_TEMP \\\n  -adv \\\n  -dr \\\n  -r $REG \\\n  -lr $LR \\\n  --max_steps $MAX_STEPS \\\n  --warm_up_steps $WARMUP \\\n  --cpu_num 2 \\\n  --test_batch_size 32 \\\n  --valid_steps $VALID_STEPS \\\n  --log_steps 100 \\\n  --save_checkpoint_steps $SAVE_STEPS \\\n  --save_path $SAVE_DIR\n\necho \"\"\necho \"==================================================\"\necho \"Training Complete!\"\necho \"Check results in: $SAVE_DIR\"\necho \"==================================================\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:49:52.250573Z","iopub.execute_input":"2026-01-03T14:49:52.250912Z","iopub.status.idle":"2026-01-03T14:49:52.257930Z","shell.execute_reply.started":"2026-01-03T14:49:52.250878Z","shell.execute_reply":"2026-01-03T14:49:52.257141Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/my_code/train_baseline.sh\n","output_type":"stream"}],"execution_count":9},{"id":"93d75482-45d7-46f5-acbf-0325c0992d19","cell_type":"code","source":"!bash /kaggle/working/my_code/train_baseline.sh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:49:54.982750Z","iopub.execute_input":"2026-01-03T14:49:54.983281Z","execution_failed":"2026-01-03T15:29:18.479Z"}},"outputs":[{"name":"stdout","text":"==================================================\nBASELINE PairRE - Static KG Approach\nDataset: ICEWS14 (ignoring timestamps)\n==================================================\n✓ Data already prepared in ./processed\n\nConfiguration:\n  Model: BaselinePairRE\n  Dimension: 500\n  Gamma: 12.0\n  Learning Rate: 0.0001\n  Batch Size: 256\n  Negative Samples: 128\n  Max Steps: 100000\n\nNOTE: This baseline IGNORES timestamps during training!\n      Evaluation uses temporal filtering for fair comparison.\n\n2026-01-03 14:49:56,804 INFO     ========================================\n2026-01-03 14:49:56,804 INFO     BASELINE PairRE (No Temporal Modeling)\n2026-01-03 14:49:56,804 INFO     ========================================\n2026-01-03 14:49:56,804 INFO     Model: BaselinePairRE\n2026-01-03 14:49:56,804 INFO     Data Path: processed\n2026-01-03 14:49:56,804 INFO     #entity: 7128\n2026-01-03 14:49:56,804 INFO     #relation: 230\n2026-01-03 14:49:56,804 INFO     NOTE: Timestamps are IGNORED during training!\n2026-01-03 14:49:56,804 INFO           But temporal filtering is used in evaluation.\n2026-01-03 14:49:56,836 INFO     Model Parameter Configuration:\n2026-01-03 14:49:56,836 INFO     Parameter gamma: torch.Size([1]), require_grad = False\n2026-01-03 14:49:56,836 INFO     Parameter embedding_range: torch.Size([1]), require_grad = False\n2026-01-03 14:49:56,836 INFO     Parameter entity_embedding: torch.Size([7128, 500]), require_grad = True\n2026-01-03 14:49:56,836 INFO     Parameter relation_embedding: torch.Size([230, 1000]), require_grad = True\n2026-01-03 14:49:58,554 INFO     Randomly Initializing BaselinePairRE Model...\n2026-01-03 14:49:58,554 INFO     Start Training...\n2026-01-03 14:49:58,554 INFO     init_step = 0\n2026-01-03 14:49:58,554 INFO     batch_size = 256\n2026-01-03 14:49:58,554 INFO     negative_adversarial_sampling = True\n2026-01-03 14:49:58,554 INFO     hidden_dim = 500\n2026-01-03 14:49:58,554 INFO     gamma = 12.000000\n2026-01-03 14:49:58,554 INFO     adversarial_temperature = 1.000000\n2026-01-03 14:49:58,555 INFO     learning_rate = 0.000100\n2026-01-03 14:49:59,285 INFO     Training positive_sample_loss at step 0: 0.000007\n2026-01-03 14:49:59,286 INFO     Training negative_sample_loss at step 0: 11.854504\n2026-01-03 14:49:59,286 INFO     Training loss at step 0: 5.927276\n2026-01-03 14:50:29,032 INFO     Training positive_sample_loss at step 100: 0.000008\n2026-01-03 14:50:29,032 INFO     Training negative_sample_loss at step 100: 11.772325\n2026-01-03 14:50:29,033 INFO     Training loss at step 100: 5.886205\n2026-01-03 14:50:58,617 INFO     Training positive_sample_loss at step 200: 0.000009\n2026-01-03 14:50:58,617 INFO     Training negative_sample_loss at step 200: 11.556669\n2026-01-03 14:50:58,617 INFO     Training loss at step 200: 5.778437\n2026-01-03 14:51:28,146 INFO     Training positive_sample_loss at step 300: 0.000012\n2026-01-03 14:51:28,146 INFO     Training negative_sample_loss at step 300: 11.247708\n2026-01-03 14:51:28,146 INFO     Training loss at step 300: 5.624070\n2026-01-03 14:51:57,722 INFO     Training positive_sample_loss at step 400: 0.000019\n2026-01-03 14:51:57,722 INFO     Training negative_sample_loss at step 400: 10.854600\n2026-01-03 14:51:57,722 INFO     Training loss at step 400: 5.427693\n2026-01-03 14:52:27,299 INFO     Training positive_sample_loss at step 500: 0.000032\n2026-01-03 14:52:27,299 INFO     Training negative_sample_loss at step 500: 10.379604\n2026-01-03 14:52:27,299 INFO     Training loss at step 500: 5.190426\n2026-01-03 14:52:56,761 INFO     Training positive_sample_loss at step 600: 0.000064\n2026-01-03 14:52:56,761 INFO     Training negative_sample_loss at step 600: 9.838113\n2026-01-03 14:52:56,761 INFO     Training loss at step 600: 4.919957\n2026-01-03 14:53:26,337 INFO     Training positive_sample_loss at step 700: 0.000155\n2026-01-03 14:53:26,337 INFO     Training negative_sample_loss at step 700: 9.214565\n2026-01-03 14:53:26,337 INFO     Training loss at step 700: 4.608524\n2026-01-03 14:53:55,882 INFO     Training positive_sample_loss at step 800: 0.000473\n2026-01-03 14:53:55,883 INFO     Training negative_sample_loss at step 800: 8.512215\n2026-01-03 14:53:55,883 INFO     Training loss at step 800: 4.257846\n2026-01-03 14:54:25,425 INFO     Training positive_sample_loss at step 900: 0.001840\n2026-01-03 14:54:25,426 INFO     Training negative_sample_loss at step 900: 7.705702\n2026-01-03 14:54:25,426 INFO     Training loss at step 900: 3.855647\n2026-01-03 14:54:55,002 INFO     Training positive_sample_loss at step 1000: 0.008686\n2026-01-03 14:54:55,002 INFO     Training negative_sample_loss at step 1000: 6.834191\n2026-01-03 14:54:55,002 INFO     Training loss at step 1000: 3.423710\n2026-01-03 14:55:24,583 INFO     Training positive_sample_loss at step 1100: 0.037342\n2026-01-03 14:55:24,583 INFO     Training negative_sample_loss at step 1100: 5.909601\n2026-01-03 14:55:24,583 INFO     Training loss at step 1100: 2.976148\n2026-01-03 14:55:54,082 INFO     Training positive_sample_loss at step 1200: 0.105198\n2026-01-03 14:55:54,082 INFO     Training negative_sample_loss at step 1200: 5.074673\n2026-01-03 14:55:54,082 INFO     Training loss at step 1200: 2.593019\n2026-01-03 14:56:23,693 INFO     Training positive_sample_loss at step 1300: 0.192944\n2026-01-03 14:56:23,693 INFO     Training negative_sample_loss at step 1300: 4.353430\n2026-01-03 14:56:23,693 INFO     Training loss at step 1300: 2.276683\n2026-01-03 14:56:53,269 INFO     Training positive_sample_loss at step 1400: 0.271582\n2026-01-03 14:56:53,269 INFO     Training negative_sample_loss at step 1400: 3.741858\n2026-01-03 14:56:53,269 INFO     Training loss at step 1400: 2.010633\n2026-01-03 14:57:22,853 INFO     Training positive_sample_loss at step 1500: 0.337168\n2026-01-03 14:57:22,853 INFO     Training negative_sample_loss at step 1500: 3.226871\n2026-01-03 14:57:22,853 INFO     Training loss at step 1500: 1.786347\n2026-01-03 14:57:52,421 INFO     Training positive_sample_loss at step 1600: 0.384973\n2026-01-03 14:57:52,421 INFO     Training negative_sample_loss at step 1600: 2.821793\n2026-01-03 14:57:52,422 INFO     Training loss at step 1600: 1.608109\n2026-01-03 14:58:22,000 INFO     Training positive_sample_loss at step 1700: 0.435866\n2026-01-03 14:58:22,000 INFO     Training negative_sample_loss at step 1700: 2.441396\n2026-01-03 14:58:22,000 INFO     Training loss at step 1700: 1.443728\n2026-01-03 14:58:51,466 INFO     Training positive_sample_loss at step 1800: 0.470576\n2026-01-03 14:58:51,467 INFO     Training negative_sample_loss at step 1800: 2.124100\n2026-01-03 14:58:51,467 INFO     Training loss at step 1800: 1.302771\n2026-01-03 14:59:20,989 INFO     Training positive_sample_loss at step 1900: 0.494866\n2026-01-03 14:59:20,989 INFO     Training negative_sample_loss at step 1900: 1.915439\n2026-01-03 14:59:20,989 INFO     Training loss at step 1900: 1.210891\n2026-01-03 14:59:50,557 INFO     Training positive_sample_loss at step 2000: 0.508318\n2026-01-03 14:59:50,558 INFO     Training negative_sample_loss at step 2000: 1.728303\n2026-01-03 14:59:50,558 INFO     Training loss at step 2000: 1.124331\n2026-01-03 15:00:20,154 INFO     Training positive_sample_loss at step 2100: 0.520764\n2026-01-03 15:00:20,154 INFO     Training negative_sample_loss at step 2100: 1.608619\n2026-01-03 15:00:20,154 INFO     Training loss at step 2100: 1.070967\n2026-01-03 15:00:49,732 INFO     Training positive_sample_loss at step 2200: 0.523116\n2026-01-03 15:00:49,732 INFO     Training negative_sample_loss at step 2200: 1.487996\n2026-01-03 15:00:49,732 INFO     Training loss at step 2200: 1.012061\n2026-01-03 15:01:19,211 INFO     Training positive_sample_loss at step 2300: 0.526714\n2026-01-03 15:01:19,212 INFO     Training negative_sample_loss at step 2300: 1.350494\n2026-01-03 15:01:19,212 INFO     Training loss at step 2300: 0.945315\n2026-01-03 15:01:48,699 INFO     Training positive_sample_loss at step 2400: 0.511738\n2026-01-03 15:01:48,699 INFO     Training negative_sample_loss at step 2400: 1.274208\n2026-01-03 15:01:48,699 INFO     Training loss at step 2400: 0.899871\n2026-01-03 15:02:18,207 INFO     Training positive_sample_loss at step 2500: 0.516967\n2026-01-03 15:02:18,207 INFO     Training negative_sample_loss at step 2500: 1.192546\n2026-01-03 15:02:18,207 INFO     Training loss at step 2500: 0.861825\n2026-01-03 15:02:47,771 INFO     Training positive_sample_loss at step 2600: 0.517985\n2026-01-03 15:02:47,771 INFO     Training negative_sample_loss at step 2600: 1.145591\n2026-01-03 15:02:47,771 INFO     Training loss at step 2600: 0.839011\n2026-01-03 15:03:17,344 INFO     Training positive_sample_loss at step 2700: 0.519730\n2026-01-03 15:03:17,344 INFO     Training negative_sample_loss at step 2700: 1.085229\n2026-01-03 15:03:17,344 INFO     Training loss at step 2700: 0.809840\n2026-01-03 15:03:46,905 INFO     Training positive_sample_loss at step 2800: 0.521492\n2026-01-03 15:03:46,906 INFO     Training negative_sample_loss at step 2800: 1.018856\n2026-01-03 15:03:46,906 INFO     Training loss at step 2800: 0.777657\n2026-01-03 15:04:16,388 INFO     Training positive_sample_loss at step 2900: 0.507006\n2026-01-03 15:04:16,388 INFO     Training negative_sample_loss at step 2900: 0.983820\n2026-01-03 15:04:16,388 INFO     Training loss at step 2900: 0.753008\n2026-01-03 15:04:45,925 INFO     Training positive_sample_loss at step 3000: 0.499025\n2026-01-03 15:04:45,925 INFO     Training negative_sample_loss at step 3000: 0.962559\n2026-01-03 15:04:45,925 INFO     Training loss at step 3000: 0.738499\n2026-01-03 15:05:15,504 INFO     Training positive_sample_loss at step 3100: 0.504817\n2026-01-03 15:05:15,504 INFO     Training negative_sample_loss at step 3100: 0.905297\n2026-01-03 15:05:15,504 INFO     Training loss at step 3100: 0.712865\n2026-01-03 15:05:45,063 INFO     Training positive_sample_loss at step 3200: 0.506027\n2026-01-03 15:05:45,063 INFO     Training negative_sample_loss at step 3200: 0.893619\n2026-01-03 15:05:45,063 INFO     Training loss at step 3200: 0.707720\n2026-01-03 15:06:14,650 INFO     Training positive_sample_loss at step 3300: 0.505551\n2026-01-03 15:06:14,650 INFO     Training negative_sample_loss at step 3300: 0.861935\n2026-01-03 15:06:14,650 INFO     Training loss at step 3300: 0.691722\n2026-01-03 15:06:44,249 INFO     Training positive_sample_loss at step 3400: 0.502898\n2026-01-03 15:06:44,249 INFO     Training negative_sample_loss at step 3400: 0.826739\n2026-01-03 15:06:44,249 INFO     Training loss at step 3400: 0.672874\n2026-01-03 15:07:13,726 INFO     Training positive_sample_loss at step 3500: 0.487564\n2026-01-03 15:07:13,726 INFO     Training negative_sample_loss at step 3500: 0.803836\n2026-01-03 15:07:13,726 INFO     Training loss at step 3500: 0.653828\n2026-01-03 15:07:43,294 INFO     Training positive_sample_loss at step 3600: 0.486685\n2026-01-03 15:07:43,294 INFO     Training negative_sample_loss at step 3600: 0.781249\n2026-01-03 15:07:43,294 INFO     Training loss at step 3600: 0.642166\n2026-01-03 15:08:12,859 INFO     Training positive_sample_loss at step 3700: 0.489997\n2026-01-03 15:08:12,859 INFO     Training negative_sample_loss at step 3700: 0.755792\n2026-01-03 15:08:12,859 INFO     Training loss at step 3700: 0.631161\n2026-01-03 15:08:42,409 INFO     Training positive_sample_loss at step 3800: 0.491075\n2026-01-03 15:08:42,409 INFO     Training negative_sample_loss at step 3800: 0.743197\n2026-01-03 15:08:42,409 INFO     Training loss at step 3800: 0.625465\n2026-01-03 15:09:11,981 INFO     Training positive_sample_loss at step 3900: 0.488108\n2026-01-03 15:09:11,981 INFO     Training negative_sample_loss at step 3900: 0.730200\n2026-01-03 15:09:11,981 INFO     Training loss at step 3900: 0.617540\n2026-01-03 15:09:41,418 INFO     Training positive_sample_loss at step 4000: 0.485946\n2026-01-03 15:09:41,418 INFO     Training negative_sample_loss at step 4000: 0.718420\n2026-01-03 15:09:41,418 INFO     Training loss at step 4000: 0.610623\n2026-01-03 15:10:10,973 INFO     Training positive_sample_loss at step 4100: 0.470068\n2026-01-03 15:10:10,973 INFO     Training negative_sample_loss at step 4100: 0.687583\n2026-01-03 15:10:10,973 INFO     Training loss at step 4100: 0.587322\n2026-01-03 15:10:40,553 INFO     Training positive_sample_loss at step 4200: 0.473856\n2026-01-03 15:10:40,553 INFO     Training negative_sample_loss at step 4200: 0.678732\n2026-01-03 15:10:40,553 INFO     Training loss at step 4200: 0.584843\n2026-01-03 15:11:10,141 INFO     Training positive_sample_loss at step 4300: 0.474002\n2026-01-03 15:11:10,142 INFO     Training negative_sample_loss at step 4300: 0.672103\n2026-01-03 15:11:10,142 INFO     Training loss at step 4300: 0.581651\n2026-01-03 15:11:39,718 INFO     Training positive_sample_loss at step 4400: 0.471111\n2026-01-03 15:11:39,719 INFO     Training negative_sample_loss at step 4400: 0.662604\n2026-01-03 15:11:39,719 INFO     Training loss at step 4400: 0.575505\n2026-01-03 15:12:09,304 INFO     Training positive_sample_loss at step 4500: 0.470273\n2026-01-03 15:12:09,304 INFO     Training negative_sample_loss at step 4500: 0.646748\n2026-01-03 15:12:09,304 INFO     Training loss at step 4500: 0.567205\n2026-01-03 15:12:38,750 INFO     Training positive_sample_loss at step 4600: 0.461941\n2026-01-03 15:12:38,750 INFO     Training negative_sample_loss at step 4600: 0.617909\n2026-01-03 15:12:38,750 INFO     Training loss at step 4600: 0.548663\n2026-01-03 15:13:08,318 INFO     Training positive_sample_loss at step 4700: 0.456910\n2026-01-03 15:13:08,318 INFO     Training negative_sample_loss at step 4700: 0.616208\n2026-01-03 15:13:08,318 INFO     Training loss at step 4700: 0.545345\n2026-01-03 15:13:37,878 INFO     Training positive_sample_loss at step 4800: 0.455866\n2026-01-03 15:13:37,878 INFO     Training negative_sample_loss at step 4800: 0.602382\n2026-01-03 15:13:37,878 INFO     Training loss at step 4800: 0.537955\n2026-01-03 15:14:07,470 INFO     Training positive_sample_loss at step 4900: 0.453223\n2026-01-03 15:14:07,470 INFO     Training negative_sample_loss at step 4900: 0.592602\n2026-01-03 15:14:07,470 INFO     Training loss at step 4900: 0.531786\n2026-01-03 15:14:37,062 INFO     Training positive_sample_loss at step 5000: 0.451446\n2026-01-03 15:14:37,062 INFO     Training negative_sample_loss at step 5000: 0.598960\n2026-01-03 15:14:37,062 INFO     Training loss at step 5000: 0.534118\n2026-01-03 15:14:37,062 INFO     Evaluating on Valid Dataset...\n2026-01-03 15:14:37,420 INFO     Evaluating the model... (0/560)\n2026-01-03 15:15:38,631 INFO     Valid MRR at step 5000: 0.191628\n2026-01-03 15:15:38,631 INFO     Valid MR at step 5000: 484.558662\n2026-01-03 15:15:38,631 INFO     Valid HITS@1 at step 5000: 0.109999\n2026-01-03 15:15:38,631 INFO     Valid HITS@3 at step 5000: 0.224248\n2026-01-03 15:15:38,631 INFO     Valid HITS@10 at step 5000: 0.341125\n2026-01-03 15:16:08,192 INFO     Training positive_sample_loss at step 5100: 0.455146\n2026-01-03 15:16:08,192 INFO     Training negative_sample_loss at step 5100: 0.580890\n2026-01-03 15:16:08,192 INFO     Training loss at step 5100: 0.526974\n2026-01-03 15:16:37,643 INFO     Training positive_sample_loss at step 5200: 0.440205\n2026-01-03 15:16:37,643 INFO     Training negative_sample_loss at step 5200: 0.568517\n2026-01-03 15:16:37,644 INFO     Training loss at step 5200: 0.513356\n2026-01-03 15:17:07,248 INFO     Training positive_sample_loss at step 5300: 0.437814\n2026-01-03 15:17:07,248 INFO     Training negative_sample_loss at step 5300: 0.558487\n2026-01-03 15:17:07,248 INFO     Training loss at step 5300: 0.507189\n2026-01-03 15:17:36,806 INFO     Training positive_sample_loss at step 5400: 0.438956\n2026-01-03 15:17:36,806 INFO     Training negative_sample_loss at step 5400: 0.546368\n2026-01-03 15:17:36,806 INFO     Training loss at step 5400: 0.501742\n2026-01-03 15:18:06,379 INFO     Training positive_sample_loss at step 5500: 0.436688\n2026-01-03 15:18:06,379 INFO     Training negative_sample_loss at step 5500: 0.546631\n2026-01-03 15:18:06,379 INFO     Training loss at step 5500: 0.500777\n2026-01-03 15:18:35,942 INFO     Training positive_sample_loss at step 5600: 0.437010\n2026-01-03 15:18:35,942 INFO     Training negative_sample_loss at step 5600: 0.542540\n2026-01-03 15:18:35,942 INFO     Training loss at step 5600: 0.498931\n2026-01-03 15:19:05,347 INFO     Training positive_sample_loss at step 5700: 0.435076\n2026-01-03 15:19:05,347 INFO     Training negative_sample_loss at step 5700: 0.535349\n2026-01-03 15:19:05,347 INFO     Training loss at step 5700: 0.494406\n2026-01-03 15:19:35,001 INFO     Training positive_sample_loss at step 5800: 0.417834\n2026-01-03 15:19:35,001 INFO     Training negative_sample_loss at step 5800: 0.521604\n2026-01-03 15:19:35,001 INFO     Training loss at step 5800: 0.478951\n2026-01-03 15:20:04,538 INFO     Training positive_sample_loss at step 5900: 0.419291\n2026-01-03 15:20:04,538 INFO     Training negative_sample_loss at step 5900: 0.521987\n2026-01-03 15:20:04,538 INFO     Training loss at step 5900: 0.479913\n2026-01-03 15:20:34,083 INFO     Training positive_sample_loss at step 6000: 0.423441\n2026-01-03 15:20:34,083 INFO     Training negative_sample_loss at step 6000: 0.496746\n2026-01-03 15:20:34,084 INFO     Training loss at step 6000: 0.469406\n2026-01-03 15:21:03,660 INFO     Training positive_sample_loss at step 6100: 0.420426\n2026-01-03 15:21:03,661 INFO     Training negative_sample_loss at step 6100: 0.507003\n2026-01-03 15:21:03,661 INFO     Training loss at step 6100: 0.473062\n2026-01-03 15:21:33,221 INFO     Training positive_sample_loss at step 6200: 0.420796\n2026-01-03 15:21:33,221 INFO     Training negative_sample_loss at step 6200: 0.500420\n2026-01-03 15:21:33,221 INFO     Training loss at step 6200: 0.469988\n2026-01-03 15:22:02,690 INFO     Training positive_sample_loss at step 6300: 0.411479\n2026-01-03 15:22:02,690 INFO     Training negative_sample_loss at step 6300: 0.493732\n2026-01-03 15:22:02,690 INFO     Training loss at step 6300: 0.462019\n2026-01-03 15:22:32,250 INFO     Training positive_sample_loss at step 6400: 0.404606\n2026-01-03 15:22:32,250 INFO     Training negative_sample_loss at step 6400: 0.489568\n2026-01-03 15:22:32,250 INFO     Training loss at step 6400: 0.456539\n2026-01-03 15:23:01,788 INFO     Training positive_sample_loss at step 6500: 0.406150\n2026-01-03 15:23:01,788 INFO     Training negative_sample_loss at step 6500: 0.476882\n2026-01-03 15:23:01,788 INFO     Training loss at step 6500: 0.451006\n2026-01-03 15:23:31,338 INFO     Training positive_sample_loss at step 6600: 0.404218\n2026-01-03 15:23:31,338 INFO     Training negative_sample_loss at step 6600: 0.477631\n2026-01-03 15:23:31,339 INFO     Training loss at step 6600: 0.450449\n2026-01-03 15:24:00,885 INFO     Training positive_sample_loss at step 6700: 0.402559\n2026-01-03 15:24:00,885 INFO     Training negative_sample_loss at step 6700: 0.467802\n2026-01-03 15:24:00,885 INFO     Training loss at step 6700: 0.444739\n2026-01-03 15:24:30,441 INFO     Training positive_sample_loss at step 6800: 0.399222\n2026-01-03 15:24:30,441 INFO     Training negative_sample_loss at step 6800: 0.471645\n2026-01-03 15:24:30,441 INFO     Training loss at step 6800: 0.445028\n2026-01-03 15:24:59,912 INFO     Training positive_sample_loss at step 6900: 0.392150\n2026-01-03 15:24:59,913 INFO     Training negative_sample_loss at step 6900: 0.459616\n2026-01-03 15:24:59,913 INFO     Training loss at step 6900: 0.435512\n2026-01-03 15:25:29,451 INFO     Training positive_sample_loss at step 7000: 0.387898\n2026-01-03 15:25:29,451 INFO     Training negative_sample_loss at step 7000: 0.459421\n2026-01-03 15:25:29,451 INFO     Training loss at step 7000: 0.433326\n2026-01-03 15:25:59,008 INFO     Training positive_sample_loss at step 7100: 0.388917\n2026-01-03 15:25:59,008 INFO     Training negative_sample_loss at step 7100: 0.447556\n2026-01-03 15:25:59,008 INFO     Training loss at step 7100: 0.427939\n2026-01-03 15:26:28,575 INFO     Training positive_sample_loss at step 7200: 0.389552\n2026-01-03 15:26:28,576 INFO     Training negative_sample_loss at step 7200: 0.445982\n2026-01-03 15:26:28,576 INFO     Training loss at step 7200: 0.427503\n2026-01-03 15:26:58,145 INFO     Training positive_sample_loss at step 7300: 0.387155\n2026-01-03 15:26:58,145 INFO     Training negative_sample_loss at step 7300: 0.438700\n2026-01-03 15:26:58,145 INFO     Training loss at step 7300: 0.422697\n2026-01-03 15:27:27,697 INFO     Training positive_sample_loss at step 7400: 0.385670\n2026-01-03 15:27:27,697 INFO     Training negative_sample_loss at step 7400: 0.444537\n2026-01-03 15:27:27,697 INFO     Training loss at step 7400: 0.424901\n2026-01-03 15:27:57,105 INFO     Training positive_sample_loss at step 7500: 0.373960\n2026-01-03 15:27:57,106 INFO     Training negative_sample_loss at step 7500: 0.428356\n2026-01-03 15:27:57,106 INFO     Training loss at step 7500: 0.410989\n2026-01-03 15:28:26,648 INFO     Training positive_sample_loss at step 7600: 0.374537\n2026-01-03 15:28:26,648 INFO     Training negative_sample_loss at step 7600: 0.431093\n2026-01-03 15:28:26,648 INFO     Training loss at step 7600: 0.412683\n2026-01-03 15:28:56,191 INFO     Training positive_sample_loss at step 7700: 0.375244\n2026-01-03 15:28:56,191 INFO     Training negative_sample_loss at step 7700: 0.421051\n2026-01-03 15:28:56,191 INFO     Training loss at step 7700: 0.408048\n","output_type":"stream"}],"execution_count":null},{"id":"ffe6b851","cell_type":"code","source":"import shutil\nimport os\n\n# Tên file zip kết quả\noutput_filename = \"/kaggle/working/ket_qua_training\"\n\ndir_to_zip = \"/kaggle/working/my_code\"\n\ntry:\n    shutil.make_archive(output_filename, 'zip', dir_to_zip)\n    print(f\"Đã nén xong! File nằm tại: {output_filename}.zip\")\nexcept Exception as e:\n    print(f\"Lỗi khi nén: {e}\")","metadata":{"execution":{"iopub.execute_input":"2025-12-29T12:06:00.425160Z","iopub.status.busy":"2025-12-29T12:06:00.424458Z","iopub.status.idle":"2025-12-29T12:06:43.061499Z","shell.execute_reply":"2025-12-29T12:06:43.060663Z"},"papermill":{"duration":43.796078,"end_time":"2025-12-29T12:06:43.638787","exception":false,"start_time":"2025-12-29T12:05:59.842709","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Đã nén xong! File nằm tại: /kaggle/working/ket_qua_training.zip\n"]}],"execution_count":13}]}